{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "1. After each stride-2 conv, why do we double the number of filters?**\n",
    "\n",
    "Doubling the number of filters after each stride-2 conv is a common\n",
    "practice to increase the number of features learned by the CNN. It helps\n",
    "to capture more complex patterns and features at a higher abstraction\n",
    "level, while also maintaining a manageable number of parameters in the\n",
    "model.\n",
    "\n",
    "**2. Why do we use a larger kernel with MNIST (with simple cnn) in the\n",
    "first conv?**\n",
    "\n",
    "Using a larger kernel in the first conv layer of MNIST CNN helps to\n",
    "capture larger patterns and features in the input image. This is because\n",
    "the first layer is responsible for detecting low-level features such as\n",
    "edges and corners, which can be better captured using a larger kernel\n",
    "size.\n",
    "\n",
    "**3. What data is saved by ActivationStats for each layer?**\n",
    "\n",
    "ActivationStats saves the maximum, minimum, and mean values of\n",
    "activations for each layer during training. This information can be\n",
    "useful for diagnosing issues with the network's learning, such as\n",
    "vanishing gradients or dead neurons.\n",
    "\n",
    "**4. How do we get a learner's callback after they've completed\n",
    "training?**\n",
    "\n",
    "To get a learner's callback after they've completed training, we can\n",
    "pass the callback function to the fit method of the learner object. The\n",
    "callback function will be executed at the end of each training epoch,\n",
    "allowing us to monitor and record various metrics.\n",
    "\n",
    "**5. What are the drawbacks of activations above zero?**\n",
    "\n",
    "The main drawback of activations above zero is the potential for\n",
    "exploding gradients, which can cause the network to diverge during\n",
    "training. This can happen when the activations become too large, leading\n",
    "to very large gradients that cause the weights to update too much, and\n",
    "the model's accuracy to decrease.\n",
    "\n",
    "**6.Draw up the benefits and drawbacks of practicing in larger\n",
    "batches?**\n",
    "\n",
    "Practicing in larger batches can have benefits such as faster training\n",
    "times and smoother convergence due to reduced variance in the gradient\n",
    "estimates. However, drawbacks include higher memory requirements,\n",
    "potential overfitting, and a reduced ability to generalize to new data.\n",
    "\n",
    "**7. Why should we avoid starting training with a high learning rate?**\n",
    "\n",
    "Starting training with a high learning rate can cause the model's\n",
    "weights to update too much and lead to unstable training. This can\n",
    "result in the model diverging, oscillating, or getting stuck in\n",
    "suboptimal solutions.\n",
    "\n",
    "**8. What are the pros of studying with a high rate of learning?**\n",
    "\n",
    "Pros of studying with a high rate of learning include faster training\n",
    "times and a greater ability to escape local minima. It can also lead to\n",
    "better generalization in some cases by promoting the model's ability to\n",
    "learn more robust features.\n",
    "\n",
    "**9. Why do we want to end the training with a low learning rate?**\n",
    "\n",
    "Ending training with a low learning rate helps to stabilize the model's\n",
    "weights and prevent them from oscillating or diverging. It can also\n",
    "improve the model's ability to generalize to new data by fine-tuning the\n",
    "weights to better fit the training data."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
